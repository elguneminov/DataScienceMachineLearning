{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Corpus**: The body/collection of text being investigated.\n",
    "* **Document**: The unit of analysis, what is considered a single observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Lets try out spacy. We can easily divide our text into sentences! I have run out of ideas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets try out spacy.\n",
      "We can easily divide our text into sentences!\n",
      "I have run out of ideas.\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lets"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "can"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ = Part of speech\n",
    "tag_ = Detailed Part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('The quick brown fox jumped over the lazy dog. Mr. Peanut wears a top hat.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'DET', 'DT')\n",
      "('quick', 'ADJ', 'JJ')\n",
      "('brown', 'ADJ', 'JJ')\n",
      "('fox', 'PROPN', 'NNP')\n",
      "('jumped', 'VERB', 'VBD')\n",
      "('over', 'ADP', 'IN')\n",
      "('the', 'DET', 'DT')\n",
      "('lazy', 'ADJ', 'JJ')\n",
      "('dog', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n",
      "('Mr.', 'PROPN', 'NNP')\n",
      "('Peanut', 'PROPN', 'NNP')\n",
      "('wears', 'VERB', 'VBZ')\n",
      "('a', 'DET', 'DT')\n",
      "('top', 'ADJ', 'JJ')\n",
      "('hat', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n"
     ]
    }
   ],
   "source": [
    "tags = set()\n",
    "\n",
    "for word in doc:\n",
    "    tags.add(word.tag_)\n",
    "    print((word.text,  word.pos_, word.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.', 'DT', 'IN', 'JJ', 'NN', 'NNP', 'VBD', 'VBZ'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NN', 'noun, singular or mass')\n",
      "('.', 'punctuation mark, sentence closer')\n",
      "('JJ', 'adjective')\n",
      "('VBD', 'verb, past tense')\n",
      "('VBZ', 'verb, 3rd person singular present')\n",
      "('NNP', 'noun, proper singular')\n",
      "('DT', 'determiner')\n",
      "('IN', 'conjunction, subordinating or preposition')\n"
     ]
    }
   ],
   "source": [
    "for tag in tags:\n",
    "    print((tag, spacy.explain(tag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.page('Python (programming language)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pages_to_sentences(*pages):\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for page in pages:\n",
    "        p = wikipedia.page(page)\n",
    "        doc = nlp(p.content)\n",
    "        sentences += [sent.text for sent in doc.sents]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_sents = pages_to_sentences('Python (programming language)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_sents = pages_to_sentences(\"Reticulated python\", \"Ball Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = lang_sents + animal_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python is an interpreted, high-level, general-purpose programming language.',\n",
       " \"Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\",\n",
       " 'Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.',\n",
       " 'Python is dynamically typed and garbage-collected.',\n",
       " 'It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented, and functional programming.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The reticulated python (Malayopython reticulatus) is a species of snake in the family Pythonidae.',\n",
       " 'The species is native to South Asia and Southeast Asia.',\n",
       " \"It is the world's longest snake and listed as least concern on the IUCN Red List because of its wide distribution.\",\n",
       " 'In several range countries, it is hunted for its skin, for use in traditional medicine, and for sale as a pet.',\n",
       " 'It is an excellent swimmer, has been reported far out at sea and has colonized many small islands within its range.\\n']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animal_sents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = bag_of_words.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<659x2824 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9141 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform` method returns a sparse matrix. A sparse matrix is a more efficient manner of storing a matrix. If a matrix has mostly zero entries, it is better to just store the non-zero entries and their occurrence, their row and column. Sparse matrices have the method `toarray()` that returns a full matrix **but** doing so may result in memory issues. Some key hyperparameters of the `CountVectorizer` are shown below:\n",
    "\n",
    "* `min_df`: only counts words that appear in a minimum number of documents.\n",
    "* `max_df`: only counts words that do not appear more than a maximum number of documents.\n",
    "* `max_features`: limits the number of generated features, based on the frequency.\n",
    "\n",
    "After fitting a `CountVectorizer` object, the following method and attribute help with determining which index belongs to which word.\n",
    "\n",
    "* `get_feature_names()`: Returns a list of words used as features. The index of the word corresponds to the column index.\n",
    "* `vocabulary_`: A dictionary mapping a word to its corresponding feature index.\n",
    "\n",
    "Let's use `vocabulary_` to determine how many times \"programming\" occurs in the documents for Python the programming language and python the animal. Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_counts = bag_of_words.transform(animal_sents)\n",
    "lang_counts = bag_of_words.transform(lang_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_index = bag_of_words.vocabulary_['programming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animal_counts.sum(axis=0)[0, prog_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_counts.sum(axis=0)[0, prog_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The HashingVectorizer transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5701525620873357580\n"
     ]
    }
   ],
   "source": [
    "print(hash('hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7273384014955944524\n"
     ]
    }
   ],
   "source": [
    "print(hash('digging'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1219583126548586098\n"
     ]
    }
   ],
   "source": [
    "print(hash('data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6912009904231611981\n",
      "8865167685245516106\n"
     ]
    }
   ],
   "source": [
    "print(hash('apple'))\n",
    "print(hash('apples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_bag_of_words = HashingVectorizer(norm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HashingVectorizer(alternate_sign=True, analyzer='word', binary=False,\n",
       "         decode_error='strict', dtype=<class 'numpy.float64'>,\n",
       "         encoding='utf-8', input='content', lowercase=True,\n",
       "         n_features=1048576, ngram_range=(1, 1), non_negative=False,\n",
       "         norm=None, preprocessor=None, stop_words=None, strip_accents=None,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_bag_of_words.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<659x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9141 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_bag_of_words.transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_wights = tfidf.fit_transform(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2017)\t0.12406941267839564\n",
      "  (0, 1999)\t0.4332472531353061\n",
      "  (0, 1977)\t0.27490568683645294\n",
      "  (0, 1464)\t0.3748599854757929\n",
      "  (0, 1428)\t0.24562367384616593\n",
      "  (0, 1359)\t0.15941849497574362\n",
      "  (0, 1344)\t0.4074104756037003\n",
      "  (0, 1202)\t0.38907899021289033\n",
      "  (0, 1101)\t0.34491072729047456\n",
      "  (0, 223)\t0.24235519670854555\n",
      "  (1, 2780)\t0.1312317717331203\n",
      "  (1, 2762)\t0.263174743750557\n",
      "  (1, 2688)\t0.22280181219440504\n",
      "  (1, 2673)\t0.18508541773543052\n",
      "  (1, 2311)\t0.25133316318455723\n",
      "  (1, 2174)\t0.22280181219440504\n",
      "  (1, 2100)\t0.2096936074270906\n",
      "  (1, 2054)\t0.25133316318455723\n",
      "  (1, 2017)\t0.08014505724364421\n",
      "  (1, 1883)\t0.22280181219440504\n",
      "  (1, 1768)\t0.09418834112336935\n",
      "  (1, 1743)\t0.27986451417470937\n",
      "  (1, 1376)\t0.14820060789454798\n",
      "  (1, 1269)\t0.09285323905135626\n",
      "  (1, 1156)\t0.24214811971573488\n",
      "  :\t:\n",
      "  (649, 2022)\t0.46695799816305983\n",
      "  (649, 1879)\t0.6768975860919657\n",
      "  (649, 331)\t0.45299375591365837\n",
      "  (649, 278)\t0.344320407466149\n",
      "  (650, 1879)\t0.6543113476481107\n",
      "  (650, 948)\t0.7562252708941385\n",
      "  (651, 2088)\t0.6038727549671913\n",
      "  (651, 2017)\t0.4125188946968684\n",
      "  (651, 461)\t0.5396625701935125\n",
      "  (651, 331)\t0.41704935877967797\n",
      "  (652, 2121)\t1.0\n",
      "  (653, 2183)\t0.5975339696744675\n",
      "  (653, 2088)\t0.5327540019071954\n",
      "  (653, 2017)\t0.3639360944906843\n",
      "  (653, 461)\t0.47610592063511287\n",
      "  (654, 2564)\t1.0\n",
      "  (655, 2589)\t0.22245127738904627\n",
      "  (655, 2203)\t0.6893893780687304\n",
      "  (655, 959)\t0.6893893780687304\n",
      "  (656, 474)\t1.0\n",
      "  (657, 2289)\t0.6889581314747405\n",
      "  (657, 2017)\t0.209809604442335\n",
      "  (657, 461)\t0.5489512933357245\n",
      "  (657, 331)\t0.42422765174328364\n",
      "  (658, 197)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_wights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_idf_idices = tfidf.idf_.argsort()[:-20:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_to_words = bag_of_words.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2824"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ind_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.799092654460526 zope\n",
      "6.799092654460526 mindstorms\n",
      "6.799092654460526 microcontrollers\n",
      "6.799092654460526 micropython\n",
      "6.799092654460526 microthreads\n",
      "6.799092654460526 mid\n",
      "6.799092654460526 midbody\n",
      "6.799092654460526 middle\n",
      "6.799092654460526 mime\n",
      "6.799092654460526 mimicking\n",
      "6.799092654460526 mimics\n",
      "6.799092654460526 mindanao\n",
      "6.799092654460526 mindoro\n",
      "6.799092654460526 minimalist\n",
      "6.799092654460526 metres\n",
      "6.799092654460526 connecting\n",
      "6.799092654460526 missouri\n",
      "6.799092654460526 mistakes\n",
      "6.799092654460526 mistaking\n"
     ]
    }
   ],
   "source": [
    "for ind in top_idf_idices:\n",
    "    print(tfidf.idf_[ind], ind_to_words[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n"
     ]
    }
   ],
   "source": [
    "print(type(STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'n‘t',\n",
       " 'n’t',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'python',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '‘d',\n",
       " '‘ll',\n",
       " '‘m',\n",
       " '‘re',\n",
       " '‘s',\n",
       " '‘ve',\n",
       " '’d',\n",
       " '’ll',\n",
       " '’m',\n",
       " '’re',\n",
       " '’s',\n",
       " '’ve'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOP_WORDS_python = STOP_WORDS.union({\"python\"})\n",
    "STOP_WORDS_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'run', 'run']\n",
      "['buy', 'buy', 'buy', 'buy']\n",
      "['see', 'see', 'see', 'see']\n"
     ]
    }
   ],
   "source": [
    "print([word.lemma_ for word in nlp('run runs ran running')])\n",
    "print([word.lemma_ for word in nlp('buy buys buying bought')])\n",
    "print([word.lemma_ for word in nlp('see saw seen seeing')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(text):\n",
    "    return [word.lemma_ for word in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_str = ' '.join(STOP_WORDS)\n",
    "\n",
    "stop_words_lemma = set(word.lemma_ for word in nlp(stop_words_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidif = TfidfVectorizer(max_features=100, stop_words= stop_words_lemma.union({'python'}), tokenizer= lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\123\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words={'no', 'elsewhere', 'two', 'i', 'without', 'therefore', 'something', 'upon', 'might', 'side', 'thru', 'could', 'hereupon', 'above', 'fifteen', 'several', 'all', 'or', 'out', 'nor', 'get', 'rather', 'off', 'only', 'anything', 'below', 'put', 'else', 'of', 're', 'about', 'own', 'must', 'thr...'also', 'should', 'thence', 'use', 'any', 'well', 'either', 'along', 'in', 'six', 'front', 'anyway'},\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function lemmatizer at 0x00000258E435C400>,\n",
       "        use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidif.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tidif.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_bigrams = CountVectorizer(max_features=100, stop_words= stop_words_lemma.union({'python'}), ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\123\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['pron'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=100, min_df=1,\n",
       "        ngram_range=(2, 2), preprocessor=None,\n",
       "        stop_words={'no', 'elsewhere', 'two', 'i', 'without', 'therefore', 'something', 'upon', 'might', 'side', 'thru', 'could', 'hereupon', 'above', 'fifteen', 'several', 'all', 'or', 'out', 'nor', 'get', 'rather', 'off', 'only', 'anything', 'below', 'put', 'else', 'of', 're', 'about', 'own', 'must', 'thr...'also', 'should', 'thence', 'use', 'any', 'well', 'either', 'along', 'in', 'six', 'front', 'anyway'},\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_bigrams.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12 september',\n",
       " '23 ft',\n",
       " '25 ft',\n",
       " 'arbitrary precision',\n",
       " 'archived original',\n",
       " 'are available',\n",
       " 'are common',\n",
       " 'are supported',\n",
       " 'are used',\n",
       " 'are written',\n",
       " 'artificial intelligence',\n",
       " 'assignment statement',\n",
       " 'auliya et',\n",
       " 'ball care',\n",
       " 'ball is',\n",
       " 'ball pythons',\n",
       " 'been killed',\n",
       " 'blah eggs',\n",
       " 'block code',\n",
       " 'boy was',\n",
       " 'classes are',\n",
       " 'code block',\n",
       " 'code is',\n",
       " 'colour pattern',\n",
       " 'design philosophy',\n",
       " 'double quote',\n",
       " 'enclosure is',\n",
       " 'et al',\n",
       " 'executes block',\n",
       " 'expressions are',\n",
       " 'external links',\n",
       " 'floating point',\n",
       " 'floor division',\n",
       " 'ft length',\n",
       " 'ft long',\n",
       " 'functional programming',\n",
       " 'guido van',\n",
       " 'had been',\n",
       " 'has been',\n",
       " 'his friends',\n",
       " 'indentation similar',\n",
       " 'indonesia was',\n",
       " 'integer division',\n",
       " 'is better',\n",
       " 'is considered',\n",
       " 'is incremented',\n",
       " 'is recommended',\n",
       " 'is true',\n",
       " 'is used',\n",
       " 'is written',\n",
       " 'isbn 978',\n",
       " 'it does',\n",
       " 'it has',\n",
       " 'it is',\n",
       " 'it was',\n",
       " 'language is',\n",
       " 'languages java',\n",
       " 'large standard',\n",
       " 'list comprehensions',\n",
       " 'longest snake',\n",
       " 'new features',\n",
       " 'number is',\n",
       " 'object oriented',\n",
       " 'old boy',\n",
       " 'operator was',\n",
       " 'oriented programming',\n",
       " 'point division',\n",
       " 'programming language',\n",
       " 'programming languages',\n",
       " 'pythons are',\n",
       " 'quote marks',\n",
       " 'reference implementation',\n",
       " 'reticulated is',\n",
       " 'reticulated pythons',\n",
       " 'royal regius',\n",
       " 'scientific computing',\n",
       " 'scripting language',\n",
       " 'selayar archipelago',\n",
       " 'september 2007',\n",
       " 'sexual maturity',\n",
       " 'similar syntax',\n",
       " 'snake was',\n",
       " 'spam eggs',\n",
       " 'standard library',\n",
       " 'start stop',\n",
       " 'steering council',\n",
       " 'sulawesi indonesia',\n",
       " 'they are',\n",
       " 'total length',\n",
       " 'uses indentation',\n",
       " 'van rossum',\n",
       " 'version number',\n",
       " 'was found',\n",
       " 'was introduced',\n",
       " 'was killed',\n",
       " 'was later',\n",
       " 'was measured',\n",
       " 'was released',\n",
       " 'web server',\n",
       " 'year old']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_bigrams.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf-idf weighting\n",
    "stop words\n",
    "words and bigrams\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf-idf weighting\n",
    "stop words\n",
    "words and bigrams\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = animal_sents + lang_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['animals'] * len(animal_sents) + ['language']*len(lang_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'animals',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language',\n",
       " 'language']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_str = ' '.join(STOP_WORDS)\n",
    "\n",
    "stop_words_lemma = set(word.lemma_ for word in nlp(stop_words_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidif = TfidfVectorizer(stop_words=stop_words_lemma, tokenizer= lemmatizer, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tidif.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9711684370257967"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = [\"My Python program is only 100 bytes long.\",\n",
    "             \"A python's bite is not venomous but still hurts.\",\n",
    "             \"I can't find the error in the python code.\",\n",
    "             \"Where is my pet python; I can't find her!\",\n",
    "             \"I use for and while loops when writing Python.\",\n",
    "             \"The python will loop and wrap itself onto me.\",\n",
    "             \"I use snake case for naming my variables.\",\n",
    "             \"My python has grown to over 10 ft long!\",\n",
    "             \"I use virtual environments to manage package versions.\",\n",
    "             \"Pythons are the largest snakes in the environment.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = ['animal', 'language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test = tidif.transform(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict_proba(transformed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_indices = (y_prob[:, 1] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Python program is only 100 bytes long.  =  language  at  68.75212565333943\n",
      "A python's bite is not venomous but still hurts.  =  animal  at  53.445442328258366\n",
      "I can't find the error in the python code.  =  language  at  75.21037288878709\n",
      "Where is my pet python; I can't find her!  =  animal  at  51.870029061448506\n",
      "I use for and while loops when writing Python.  =  language  at  82.5001339993406\n",
      "The python will loop and wrap itself onto me.  =  language  at  67.06073182840161\n",
      "I use snake case for naming my variables.  =  language  at  60.675321671352364\n",
      "My python has grown to over 10 ft long!  =  animal  at  58.22505020470826\n",
      "I use virtual environments to manage package versions.  =  language  at  78.41673587332183\n",
      "Pythons are the largest snakes in the environment.  =  animal  at  68.39335550179028\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(predicted_indices):\n",
    "    print(test_docs[i], \" = \", class_label[index], ' at ', 100*y_prob[i, index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
